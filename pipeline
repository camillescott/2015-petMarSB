#!/usr/bin/env python
from __future__ import print_function

from peasoup import task_funcs
from peasoup.tasks import BlastTask, BlastFormatTask, CurlTask, GunzipTask, \
                            UniProtQueryTask, TruncateFastaNameTask

import argparse
import os
import sys
import json
import pprint
from itertools import izip

import pandas as pd

from doit.tools import run_once, create_folder, title_with_actions
from doit.task import clean_targets, dict_to_task

BOWTIE_THREADS = 1

'''
Define relevant task functions for pipeline
'''

@task_funcs.create_task_object
def diginorm_task(input_files, ksize, htsize, cutoff,
                  n_tables=4, ht_outfn=None, label=None):

    if not label:
        label = 'normalize_by_median_'
        label += '_'.join(input_files[:5])
        if len(input_files) > 5:
            label += '_and_' + str(len(input_files) - 5) + '_more'

    if len(input_files) == 1:
        report_fn = input_files[0] + '.keep.report.txt'
    else:
        report_fn = label + '.report.txt'

    inputs = ' '.join(input_files)
    ht_out_str = ''
    if ht_outfn is not None:
        ht_out_str = '-s ' + ht_outfn
    cmd = 'normalize-by-median.py -k {ksize} -x {htsize} -N {n_tables} '\
          '-C {cutoff} -R {report_fn} {ht_out_str} {inputs}'.format(**locals())

    targets = [fn + '.keep' for fn in input_files]
    targets.append(report_fn)
    if ht_out_str:
        targets.append(ht_outfn)

    return {'title': title_with_actions,
            'name': label,
            'actions': [cmd],
            'file_dep': input_files,
            'targets': targets,
            'clean': [clean_targets]}

@task_funcs.create_task_object
def filter_abund_task(input_files, ct_file, minabund, coverage, label=None):

    if not label:
        label = 'filter_abund_'
        label += '_'.join(input_files[:5])
        if len(input_files) > 5:
            label += '_and_' + str(len(input_files) - 5) + '_more'

    inputs = ' '.join(input_files)
    cmd = 'filter-abund.py -C {minabund} -V -Z {coverage} '\
          '{ct_file} {inputs}'.format(**locals())

    targets = [fn + '.abundfilt' for fn in input_files]

    return {'title': title_with_actions,
            'name': label,
            'actions': [cmd],
            'file_dep': input_files + [ct_file],
            'targets': targets,
            'clean': [clean_targets]}

@task_funcs.create_task_object
def curl_task(row):
    cmd = 'curl -o {target_fn} {url}'.format(target_fn=row.filename+'.gz', url=row.url)
    tsk = task_funcs.general_cmdline_task([], [row.filename+'.gz'], cmd)
    tsk['uptodate'] = [run_once]
    return tsk


@task_funcs.create_task_object
def create_folder_task(folder, label=''):

    if not label:
        label = 'create_folder_{folder}'.format(**locals())

    return {'title': title_with_actions,
            'name': label,
            'actions': [(create_folder, [folder])],
            'targets': [folder],
            'uptodate': [run_once],
            'clean': [clean_targets] }

def blast_task(row, config, assembly):

    blast_threads = config['pipeline']['blast']['threads']
    blast_params = config['pipeline']['blast']['params']
    blast_evalue = config['pipeline']['blast']['evalue']

    db_name = row.filename + '.db'

    t1 = '{0}.x.{1}.tsv'.format(assembly, db_name)
    t2 = '{0}.x.{1}.tsv'.format(db_name, assembly)


    if row.db_type == 'prot':
        yield BlastTask('blastx', assembly, db_name, t1,
                        num_threads=blast_threads, evalue=blast_evalue,
                        params=blast_params).tasks().next()
        yield BlastTask('tblastn', row.filename, '{}.db'.format(assembly),
                        t2, num_threads=blast_threads, evalue=blast_evalue,
                        params=blast_params).tasks().next()
    else:
        yield BlastTask('blastn', assembly, db_name, t1,
                        num_threads=blast_threads, evalue=blast_evalue,
                        params=blast_params).tasks().next()
        yield BlastTask('blastn', row.filename, '{}.db'.format(assembly),
                        t2, num_threads=blast_threads, evalue=blast_evalue,
                        params=blast_params).tasks().next()

@task_funcs.create_task_object
def link_files_task(src):
    cmd = 'ln -fs {src}'.format(src=src)
    return task_funcs.general_cmdline_task([src], [os.path.basename(src)], cmd)

@task_funcs.create_task_object
def bowtie2_build_task(row):
    return task_funcs.bowtie2_build_task(row.filename, row.filename.split('.fasta')[0])

@task_funcs.create_task_object
def split_pairs_task(row):
    return task_funcs.split_pairs_task(row.filename)

@task_funcs.create_task_object
def bowtie2_align_task(row, idx_fn):
    target = '{sample}.x.{idx}'.format(sample=row.filename, idx=idx_fn)
    encoding = '--' + row.phred
    if row.paired == False:
        return task_funcs.bowtie2_align_task(idx_fn, target, singleton_fn=row.filename,
                                             num_threads=BOWTIE_THREADS, encoding=encoding)
    else:
        return task_funcs.bowtie2_align_task(idx_fn, target, left_fn=row.filename+'.1',
                                                right_fn=row.filename + '.2', singleton_fn=row.filename + '.0',
                                                num_threads=BOWTIE_THREADS, encoding=encoding)

@task_funcs.create_task_object
def express_task(hits_fn, transcripts_fn):
    folder = hits_fn.split('.bam')[0]
    return task_funcs.eXpress_task(transcripts_fn, hits_fn, folder)

@task_funcs.create_task_object
def samtools_sort_task(hits_fn):
    return task_funcs.samtools_sort_task(hits_fn)

@task_funcs.create_task_object
def aggregate_express_task(results_files, tpm_target_fn, eff_target_fn, tot_target_fn, label=''):

    import csv

    def recursive_open(file_list, fp_dict, call, *args, **kwargs):
        if not file_list:
            call(fp_dict, *args, **kwargs)
        else:
            fn = file_list.pop()
            with open(fn, 'rb') as fp:
                fp_dict[fn] = fp
                recursive_open(file_list, fp_dict, call, *args, **kwargs)

    def cmd(results_files, tpm_target, eff_target, tot_target):
        with open(tpm_target, 'wb') as tpm_fp, \
             open(eff_target, 'wb') as eff_fp, \
             open(tot_target, 'wb') as tot_fp:

            def agg(fp_dict, tpm_fp, eff_fp, tot_fp):
                readers = [csv.DictReader(fp, delimiter='\t') for fp in fp_dict.values()]
                names = [fn + sep for fn, sep, _ in map(lambda fn: fn.partition('.fq.gz'), fp_dict.keys())]
                print(names)

                tpm_writer = csv.DictWriter(tpm_fp, ['target_id'] + names, delimiter='\t')
                tpm_writer.writeheader()

                eff_writer = csv.DictWriter(eff_fp, ['target_id'] + names, delimiter='\t')
                eff_writer.writeheader()

                tot_writer = csv.DictWriter(tot_fp, ['target_id'] + names, delimiter='\t')
                tot_writer.writeheader()

                for lines in izip(*readers):
                    data = {fn:d['tpm'] for fn, d in zip(names, lines)}
                    data['target_id'] = lines[0]['target_id']
                    tpm_writer.writerow(data)

                    data = {fn:d['eff_counts'] for fn, d in zip(names, lines)}
                    data['target_id'] = lines[0]['target_id']
                    eff_writer.writerow(data)

                    data = {fn:d['tot_counts'] for fn, d in zip(names, lines)}
                    data['target_id'] = lines[0]['target_id']
                    tot_writer.writerow(data)

            fpd = {}
            recursive_open(results_files, fpd, agg, tpm_fp, eff_fp, tot_fp)

    if not label:
        label = 'aggregate_express_task_' + os.path.basename(tpm_target_fn)

    return {'name': label,
            'title': title_with_actions,
            'actions': [(cmd, [results_files, tpm_target_fn, eff_target_fn, tot_target_fn])],
            'targets': [eff_target_fn, tpm_target_fn, tot_target_fn],
            'clean': [clean_targets],
            'file_dep': results_files}


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--resources-metadata', default='resources-test.json')
    parser.add_argument('--config-metadata', default='config-test.json')
    parser.add_argument('--print-tasks', action='store_true', default=False)
    parser.add_argument('--local-file-dir', default='_data')
    args, doit_args = parser.parse_known_args()

    with open(args.resources_metadata, 'r') as fp:
        print('** Using data resources found in {c}'.format(c=args.resources_metadata), file=sys.stderr)
        resources = json.load(fp)
    with open(args.config_metadata, 'r') as fp:
        print('** Using config found in {c}'.format(c=args.config_metadata), file=sys.stderr)
        config = json.load(fp)

    desc = '''
####################################################################
#
# 2015 Petromyzon marinus (sea lamprey) de novo RNA-seq Pipeline
#
# * Authors:
#   {authors}
#
# * About:
#   {desc}
#
####################################################################
'''.format(authors=', '.join(config['meta']['authors']),
           desc=config['meta']['description'])
    print(desc, file=sys.stderr)

    work_dir = config['pipeline']['work_dir']
    local_dir = os.path.abspath(args.local_file_dir)

    resources_df = pd.DataFrame(resources).transpose()
    sample_df = resources_df[resources_df.meta_type == 'sample']

    old_dir = os.getcwd()
    try:
        if not os.path.exists(work_dir):
            os.makedirs(work_dir)
        os.chdir(work_dir)
        print('** Current Working Directory: {w}'.format(w=os.getcwd()), file=sys.stderr)

        tasks = []

        '''
        Sample preprocessing
        '''

        link_tasks = sample_df.filename.apply(lambda fn: os.path.join(local_dir, fn)).apply(link_files_task)
        tasks.extend(list(link_tasks))

        # Run diginorm in parallel on all samples
        ksize = config['pipeline']['khmer']['ksize']
        htsize = config['pipeline']['khmer']['parallel']['table_size']
        ntables = config['pipeline']['khmer']['parallel']['n_tables']
        coverage = config['pipeline']['khmer']['parallel']['coverage']
        dg_tasks = sample_df.apply(
                            lambda row: diginorm_task([row.filename], ksize,
                                htsize, coverage, ntables), axis=1, reduce=False)
        tasks.extend(list(dg_tasks))
        dg_outputs = sample_df.filename.apply(lambda fn: fn+'.keep')

        htsize = config['pipeline']['khmer']['pooled']['table_size']
        ntables = config['pipeline']['khmer']['pooled']['n_tables']
        coverage = config['pipeline']['khmer']['pooled']['coverage']
        ht_fn = config['pipeline']['prefix'] + '.pooled.ct'
        dg_task = diginorm_task(list(dg_outputs), ksize, htsize, coverage,
                                n_tables=ntables, ht_outfn=ht_fn)
        tasks.append(dg_task)
        dg_outputs = dg_outputs.apply(lambda fn: fn+'.keep')

        minabund = config['pipeline']['khmer']['parallel']['min_abund']
        abf_task = filter_abund_task(list(dg_outputs), ht_fn, minabund, coverage)
        tasks.append(abf_task)


        '''
        Database prep, homology search
        '''
        # Use curl to get remote flat files
        remote_files = resources_df[resources_df.access == 'remote_file']
        curl_tasks = remote_files.apply(lambda row: curl_task(row),
                                        axis=1, reduce=False)
        tasks.extend(list(curl_tasks))

        # Programmatically query uniprot
        uniprot_queries = resources_df[(resources_df.access == 'remote_query') & \
                                       (resources_df.q_type == 'uniprot')]
        uniprot_tasks = uniprot_queries.apply(lambda row: dict_to_task(
                                task_funcs.uniprot_query_task(
                                                        row.terms,
                                                        row.filename+'.gz')
                                ), axis=1, reduce=False)
        tasks.extend(list(uniprot_tasks))

        # unpack gzip'd remote flat files and queries
        gunzip_df = resources_df[(resources_df.access == 'remote_query') |
                                 (resources_df.access == 'remote_file')]
        tasks.extend([dict_to_task(tsk) for tsk in
                      GunzipTask([(row.filename+'.gz', row.filename) for
                                   _, row in gunzip_df.iterrows()]).tasks()])

        # Truncate the long fasta names so blast doesn't choke
        tasks.extend(list(gunzip_df[gunzip_df.meta_type != 'gtf_database'].apply(
                        lambda row: dict_to_task(task_funcs.truncate_fasta_header_task(row.filename)),
                        axis=1, reduce=False)))

        # Generate blast indices
        tasks.extend(list(resources_df[resources_df.meta_type.isin(['fasta_database', 'assembly'])].apply(
                        lambda row: dict_to_task(task_funcs.blast_format_task(
                            row.filename, row.filename+'.db', row.db_type)),
                            axis=1, reduce=False)))

        # Run blast
        blast_iters = []
        for fn in resources_df[resources_df.meta_type == 'assembly'].filename:

            blast_iters.extend([blast_task(row, config, fn) \
                        for _, row in resources_df[(resources_df.meta_type == 'fasta_database') &
                                                   (resources_df.meta_type != 'assembly')].iterrows()])

        blast_iters.extend([blast_task(row, config, resources_df.ix['petMar2_cdna'].filename) \
                        for _, row in resources_df[(resources_df.meta_type == 'fasta_database') &
                                                   (resources_df.filename != 'petMar2.cdna.fa')].iterrows()])

        for tskiter in blast_iters:
            tasks.extend([dict_to_task(tsk) for tsk in tskiter])

        '''
        Alignment / abundance estimation stuff
        '''
        bt_build_tasks = resources_df[resources_df.meta_type == 'assembly'].apply(bowtie2_build_task, axis=1, reduce=False)
        index_basenames = resources_df[resources_df.meta_type == 'assembly'].filename.apply(lambda fn: fn.rstrip('.fasta'))
        tasks.extend(list(bt_build_tasks))

        split_tasks = sample_df[sample_df.paired == True].apply(split_pairs_task, axis=1, reduce=False)
        tasks.extend(list(split_tasks))

        for assembly in list(resources_df[resources_df.meta_type == 'assembly'].filename):
            index_fn = assembly.rstrip('.fasta')
            align_tasks = list(sample_df.apply(bowtie2_align_task, args=(index_fn,), axis=1, reduce=False))
            tasks.extend(align_tasks)

            align_files_df = task_funcs.get_task_files_df(align_tasks)
            bam_files_to_sort = align_files_df[align_files_df.apply(lambda row: row.dep.endswith('.1') and row.target.endswith('.bam'), axis=1)].target
            bam_files_single = align_files_df[align_files_df.apply(lambda row: row.dep.endswith('.fq.gz') and row.target.endswith('.bam'), axis=1)].target
            sort_tasks = bam_files_to_sort.apply(samtools_sort_task)
            tasks.extend(sort_tasks)

            hits_files = bam_files_single.append(sort_tasks.apply(lambda t: t.targets[0]))
            express_tasks = list(hits_files.apply(express_task, args=(assembly,)))
            tasks.extend(express_tasks)

            # The results.xprs file is always in position 1 in the targets --
            # kludgy, fix this later...
            express_files = [t.targets[1] for t in express_tasks]
            agg_task = aggregate_express_task(express_files, '{pref}.eXpress.tpm.tsv'.format(pref=index_fn),
                                                         '{pref}.eXpress.eff.tsv'.format(pref=index_fn),
                                                         '{pref}.eXpress.tot.tsv'.format(pref=index_fn))
            tasks.append(agg_task)

        if args.print_tasks:
            for task in tasks:
                print('-----\n', task)
                pprint.pprint(task.__dict__)

        task_funcs.run_tasks(tasks, doit_args)

    finally:
        os.chdir(old_dir)

main()
